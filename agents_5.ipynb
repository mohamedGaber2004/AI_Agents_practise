{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "21a6f102",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from langgraph.graph.message import add_messages\n",
    "from pprint import pprint\n",
    "from langchain_core.messages import AIMessage,HumanMessage , AnyMessage\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from langchain_groq import ChatGroq\n",
    "from typing_extensions import TypedDict\n",
    "from typing import Annotated\n",
    "from langgraph.graph import StateGraph , START , END\n",
    "\n",
    "from langchain_community.tools import ArxivQueryRun , WikipediaQueryRun\n",
    "from langchain_community.utilities import ArxivAPIWrapper , WikipediaAPIWrapper\n",
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "\n",
    "from langgraph.prebuilt import ToolNode , tools_condition\n",
    "from IPython.display import Image , display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f0a1db64",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c9fcd80e",
   "metadata": {},
   "outputs": [],
   "source": [
    "arxiv_wrapper = ArxivAPIWrapper(top_k_results=2,doc_content_chars_max=500)\n",
    "arxiv = ArxivQueryRun(api_wrapper=arxiv_wrapper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8e1c1ffa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Published: 2021-05-06\\nTitle: Do You Even Need Attention? A Stack of Feed-Forward Layers Does Surprisingly Well on ImageNet\\nAuthors: Luke Melas-Kyriazi\\nSummary: The strong performance of vision transformers on image classification and other vision tasks is often attributed to the design of their multi-head attention layers. However, the extent to which attention is responsible for this strong performance remains unclear. In this short report, we ask: is the attention layer even necessary? Specifi'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arxiv.invoke(\"Attention is all you need\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3cc336eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki_wrapper = WikipediaAPIWrapper(top_k_results=1,doc_content_chars_max=500)\n",
    "wiki = WikipediaQueryRun(api_wrapper=wiki_wrapper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b20f5863",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Page: Attention Is All You Need\\nSummary: \"Attention Is All You Need\" is a 2017 landmark research paper in machine learning authored by eight scientists working at Google. The paper introduced a new deep learning architecture known as the transformer, based on the attention mechanism proposed in 2014 by Bahdanau et al. It is considered a foundational paper in modern artificial intelligence, and a main contributor to the AI boom, as the transformer approach has become the main architecture of a wi'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wiki.invoke(\"Attention is all you need\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0e694c03",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mg735\\AppData\\Local\\Temp\\ipykernel_11432\\2290291758.py:1: LangChainDeprecationWarning: The class `TavilySearchResults` was deprecated in LangChain 0.3.25 and will be removed in 1.0. An updated version of the class exists in the `langchain-tavily package and should be used instead. To use it run `pip install -U `langchain-tavily` and import as `from `langchain_tavily import TavilySearch``.\n",
      "  tavily = TavilySearchResults()\n"
     ]
    }
   ],
   "source": [
    "tavily = TavilySearchResults()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bf1fd096",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'title': 'Attention Is All You Need - Wikipedia',\n",
       "  'url': 'https://en.wikipedia.org/wiki/Attention_Is_All_You_Need',\n",
       "  'content': '\"Attention Is All You Need\" is a 2017 landmark research paper in machine learning authored by eight scientists working at Google. The paper introduced a new deep learning architecture known as the transformer \"Transformer (machine learning model)\"), based on the attention mechanism proposed in 2014 by Bahdanau et al. It is considered a foundational paper in modern artificial intelligence, and a main contributor to the AI boom, as the transformer approach has become the main architecture of a [...] 1. ^ a b c d e f Vaswani, Ashish; Shazeer, Noam; Parmar, Niki; Uszkoreit, Jakob; Jones, Llion; Gomez, Aidan N; Kaiser, Łukasz; Polosukhin, Illia (December 2017). \"Attention is All you Need\" (PDF). In I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett (ed.). 31st Conference on Neural Information Processing Systems (NIPS). Advances in Neural Information Processing Systems. Vol. 30. Curran Associates, Inc. arXiv \"ArXiv [...] In 2017, the original (100M-sized) encoder–decoder transformer model was proposed in the \"Attention is all you need\" paper. At the time, the focus of the research was on improving seq2seq for machine translation, by removing its recurrence to process all tokens in parallel, but preserving its dot-product attention mechanism to keep its text processing performance. This led to the introduction of a multi-head attention model that was easier to parallelize due to the use of independent heads and',\n",
       "  'score': 0.91175514},\n",
       " {'title': '[1706.03762] Attention Is All You Need - arXiv',\n",
       "  'url': 'https://arxiv.org/abs/1706.03762',\n",
       "  'content': 'We gratefully acknowledge support from the Simons Foundation, member institutions, and all contributors. Donate\\n\\n> cs > arXiv:1706.03762\\n\\n# Computer Science > Computation and Language\\n\\narXiv:1706.03762 (cs)\\n\\nSubmitted on 12 Jun 2017 ([v1), last revised 2 Aug 2023 (this version, v7)]\\n\\n# Title:Attention Is All You Need\\n\\nAuthors:Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin [...] From: Llion Jones [view email]   \\n (/abs/1706.03762v1) Mon, 12 Jun 2017 17:57:34 UTC (1,102 KB)  \\n (/abs/1706.03762v2) Mon, 19 Jun 2017 16:49:45 UTC (1,125 KB)  \\n (/abs/1706.03762v3) Tue, 20 Jun 2017 05:20:02 UTC (1,125 KB)  \\n (/abs/1706.03762v4) Fri, 30 Jun 2017 17:29:30 UTC (1,124 KB)  \\n (/abs/1706.03762v5) Wed, 6 Dec 2017 03:30:32 UTC (1,124 KB)  \\n (/abs/1706.03762v6) Mon, 24 Jul 2023 00:48:54 UTC (1,124 KB)  \\n [v7] Wed, 2 Aug 2023 00:41:18 UTC (1,124 KB)\\n\\nFull-text links:\\n\\n## Access Paper: [...] |  |  |\\n --- |\\n| Comments: | 15 pages, 5 figures |\\n| Subjects: | Computation and Language (cs.CL); Machine Learning (cs.LG) |\\n| Cite as: | arXiv:1706.03762 [cs.CL] |\\n|  | (or  arXiv:1706.03762v7 [cs.CL] for this version) |\\n|  |  arXiv-issued DOI via DataCite |\\n\\n## Submission history',\n",
       "  'score': 0.7634314},\n",
       " {'title': '[PDF] Attention is All you Need - Semantic Scholar',\n",
       "  'url': 'https://www.semanticscholar.org/paper/Attention-is-All-you-Need-Vaswani-Shazeer/204e3073870fae3d05bcbc2f6a8e263d9b72e776',\n",
       "  'content': 'Skip to search formSkip to account menu\\n\\n Corpus ID: 13756489\\n\\n# Attention is All you Need\\n\\n```\\n@inproceedings{Vaswani2017AttentionIA, title={Attention is All you Need}, author={Ashish Vaswani and Noam Shazeer and Niki Parmar and Jakob Uszkoreit and Llion Jones and Aidan N. Gomez and Lukasz Kaiser and Illia Polosukhin}, booktitle={Neural Information Processing Systems}, year={2017}, url={ }\\n``` [...] Ashish Vaswani, Noam Shazeer,  I. Polosukhin\\n Published in Neural Information Processing… 12 June 2017\\n Computer Science\\n\\nA new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely is proposed, which generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.\\n\\n[PDF] Semantic Reader\\n\\n155,515 Citations\\n\\nHighly Influential Citations [...] Romain PaulusCaiming XiongR. Socher\\n\\nComputer Science\\n\\nICLR\\n\\n 2018\\n\\nA neural network model with a novel intra-attention that attends over the input and continuously generated output separately, and a new training method that combines standard supervised word prediction and reinforcement learning (RL) that produces higher quality summaries.\\n\\n 1,615\\n[PDF]\\n\\n### Can Active Memory Replace Attention?\\n\\nLukasz KaiserSamy Bengio\\n\\nComputer Science\\n\\nNIPS\\n\\n 2016',\n",
       "  'score': 0.76279575},\n",
       " {'title': '[PDF] Attention is All you Need - NIPS papers',\n",
       "  'url': 'https://papers.neurips.cc/paper/7181-attention-is-all-you-need.pdf',\n",
       "  'content': 'Self-attention, sometimes called intra-attention is an attention mechanism relating different positions of a single sequence in order to compute a representation of the sequence. Self-attention has been used successfully in a variety of tasks including reading comprehension, abstractive summarization, textual entailment and learning task-independent sentence representations [4, 22, 23, 19]. [...] 3.2 Attention An attention function can be described as mapping a query and a set of key-value pairs to an output, where the query, keys, values, and output are all vectors. The output is computed as a weighted sum of the values, where the weight assigned to each value is computed by a compatibility function of the query with the corresponding key. [...] In this work we propose the Transformer, a model architecture eschewing recurrence and instead relying entirely on an attention mechanism to draw global dependencies between input and output.\\nThe Transformer allows for signiﬁcantly more parallelization and can reach a new state of the art in translation quality after being trained for as little as twelve hours on eight P100 GPUs.',\n",
       "  'score': 0.75659186},\n",
       " {'title': 'Attention is all you need | Proceedings of the 31st International ...',\n",
       "  'url': 'https://dl.acm.org/doi/10.5555/3295222.3295349',\n",
       "  'content': 'skip to main content\\n\\nAdvanced Search\\n\\n10.5555/3295222.3295349guideproceedingsArticle/Chapter ViewAbstractPublication PagesnipsConference Proceedingsconference-collections\\n\\nnips\\n\\n## Export Citations\\n\\nFree access\\n\\nShare on\\n\\n# Attention is all you need\\n\\nAuthors: Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, + 4, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin (Less)Authors Info & Claims [...] Google Scholar\\n\\nMinh-Thang Luong, Hieu Pham, and Christopher D Manning. Effective approaches to attention-based neural machine translation. arXiv preprint arXiv:1508.04025, 2015.\\n\\nGoogle Scholar\\n\\nAnkur Parikh, Oscar Täckström, Dipanjan Das, and Jakob Uszkoreit. A decomposable attention model. In Empirical Methods in Natural Language Processing, 2016.\\n\\nGoogle Scholar [...] Google Scholar\\n\\nSainbayar Sukhbaatar, arthur szlam, Jason Weston, and Rob Fergus. End-to-end memory networks. In C. Cortes, N. D. Lawrence, D. D. Lee, M. Sugiyama, and R. Garnett, editors, Advances in Neural Information Processing Systems 28, pages 2440-2448. Curran Associates, Inc., 2015.\\n\\nDigital Library\\n\\nGoogle Scholar',\n",
       "  'score': 0.69795954}]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tavily.invoke(\"Attention is all you need\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "875d3e7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "tools_list = [arxiv,wiki,tavily]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d5ce20b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatGroq(\n",
    "    model = \"openai/gpt-oss-120b\",\n",
    "    temperature=0\n",
    ")\n",
    "\n",
    "llm_with_tools = llm.bind_tools(tools_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4f64c04b",
   "metadata": {},
   "outputs": [],
   "source": [
    "res = llm_with_tools.invoke([HumanMessage(content=\"What is recent ai news ?\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e4597fbe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'name': 'tavily_search_results_json',\n",
       "  'args': {'query': 'AI news December 2025'},\n",
       "  'id': 'fc_796bb62f-f051-4320-98a7-5fd02c29349d',\n",
       "  'type': 'tool_call'}]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res.tool_calls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6b9ce389",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='', additional_kwargs={'reasoning_content': 'The user asks: \"What is recent ai news ?\" They want recent AI news. Need to provide up-to-date information. As of current date 2025-12-13. Need to browse recent AI news. Use search tool. Probably use tavily_search_results_json for news. Let\\'s search query \"latest AI news December 2025\".', 'tool_calls': [{'id': 'fc_796bb62f-f051-4320-98a7-5fd02c29349d', 'function': {'arguments': '{\"query\":\"AI news December 2025\"}', 'name': 'tavily_search_results_json'}, 'type': 'function'}]}, response_metadata={'token_usage': {'completion_tokens': 107, 'prompt_tokens': 291, 'total_tokens': 398, 'completion_time': 0.241662309, 'completion_tokens_details': {'reasoning_tokens': 71}, 'prompt_time': 0.012257679, 'prompt_tokens_details': None, 'queue_time': 0.038752535, 'total_time': 0.253919988}, 'model_name': 'openai/gpt-oss-120b', 'system_fingerprint': 'fp_ef479defa8', 'service_tier': 'on_demand', 'finish_reason': 'tool_calls', 'logprobs': None, 'model_provider': 'groq'}, id='lc_run--019b18dd-5c88-7441-aa03-30f5188326c4-0', tool_calls=[{'name': 'tavily_search_results_json', 'args': {'query': 'AI news December 2025'}, 'id': 'fc_796bb62f-f051-4320-98a7-5fd02c29349d', 'type': 'tool_call'}], usage_metadata={'input_tokens': 291, 'output_tokens': 107, 'total_tokens': 398, 'output_token_details': {'reasoning': 71}})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5ebef180",
   "metadata": {},
   "outputs": [],
   "source": [
    "class State (TypedDict) : \n",
    "    messages : Annotated[list[AnyMessage],add_messages]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "afaa51d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def llm_node (state:State) : \n",
    "    return {\"messages\" : [llm_with_tools.invoke(state['messages'])]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ed592db0",
   "metadata": {},
   "outputs": [],
   "source": [
    "graph = StateGraph(State)\n",
    "\n",
    "graph.add_node(\"llm_node\", llm_node)\n",
    "graph.add_node(\"tools\", ToolNode(tools_list))\n",
    "\n",
    "graph.add_edge(START, \"llm_node\")\n",
    "graph.add_conditional_edges(\n",
    "    \"llm_node\",\n",
    "    tools_condition,   # function\n",
    "    {\n",
    "        \"tools\": \"tools\",\n",
    "        \"__end__\": END\n",
    "    }\n",
    ") \n",
    "# for Basic Agent ...\n",
    "#graph.add_edge(\"tools\", END)\n",
    "# for ReAct Agent ...\n",
    "graph.add_edge(\"tools\", \"llm_node\")\n",
    "\n",
    "\n",
    "memory = MemorySaver()\n",
    "graph = graph.compile(checkpointer=memory)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f20c46af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(Image(graph.get_graph().draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "493aefa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\"configurable\":{\"thread_id\" : \"1\"}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "009b15df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "1706.03762\n",
      "None\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Tool Calls:\n",
      "  arxiv (fc_f1828abb-3273-4a0e-b207-c5ff85cebf97)\n",
      " Call ID: fc_f1828abb-3273-4a0e-b207-c5ff85cebf97\n",
      "  Args:\n",
      "    query: 1706.03762\n",
      "None\n",
      "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
      "Name: arxiv\n",
      "\n",
      "Published: 2023-08-02\n",
      "Title: Attention Is All You Need\n",
      "Authors: Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin\n",
      "Summary: The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, base\n",
      "None\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "**Paper:** *Attention Is All You Need*  \n",
      "**arXiv ID:** [1706.03762](https://arxiv.org/abs/1706.03762)  \n",
      "**Authors:** Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin  \n",
      "**Submitted:** June 2017 (published in *NeurIPS 2017*)  \n",
      "\n",
      "---\n",
      "\n",
      "## 1. What problem does the paper address?\n",
      "\n",
      "Sequence‑to‑sequence (seq2seq) models for tasks such as machine translation, summarization, and speech recognition have traditionally relied on **recurrent neural networks (RNNs)** or **convolutional neural networks (CNNs)**.  \n",
      "These architectures suffer from:\n",
      "\n",
      "* **Limited parallelism** – RNNs process tokens sequentially, slowing training.  \n",
      "* **Long‑range dependency bottlenecks** – Capturing relationships between distant tokens requires many recurrent steps.  \n",
      "* **Complexity of stacking many layers** – Deep RNNs become hard to train and memory‑intensive.\n",
      "\n",
      "The authors ask: *Can we dispense with recurrence and convolution altogether and still achieve state‑of‑the‑art performance?*  \n",
      "\n",
      "---\n",
      "\n",
      "## 2. Core idea – the **Transformer** architecture\n",
      "\n",
      "The Transformer replaces recurrence and convolution with **self‑attention** (also called *scaled dot‑product attention*) as the sole means of mixing information across positions.\n",
      "\n",
      "### 2.1. Scaled Dot‑Product Attention\n",
      "Given queries **Q**, keys **K**, and values **V**:\n",
      "\n",
      "\\[\n",
      "\\text{Attention}(Q,K,V)=\\text{softmax}\\!\\Big(\\frac{QK^{\\top}}{\\sqrt{d_k}}\\Big)V\n",
      "\\]\n",
      "\n",
      "* The dot‑product measures similarity between a query and each key.  \n",
      "* Division by \\(\\sqrt{d_k}\\) stabilizes gradients.  \n",
      "* Softmax yields a distribution over all positions, allowing each token to attend to every other token.\n",
      "\n",
      "### 2.2. Multi‑Head Attention\n",
      "Instead of a single attention function, the model uses **h** parallel heads:\n",
      "\n",
      "\\[\n",
      "\\text{MultiHead}(Q,K,V)=\\text{Concat}(\\text{head}_1,\\dots,\\text{head}_h)W^O\n",
      "\\]\n",
      "\n",
      "Each head linearly projects Q, K, V into different subspaces, learns distinct relational patterns, and the concatenated result is projected back to the model dimension.\n",
      "\n",
      "### 2.3. Position‑wise Feed‑Forward Networks (FFN)\n",
      "After attention, each position passes through the same two‑layer fully‑connected network:\n",
      "\n",
      "\\[\n",
      "\\text{FFN}(x)=\\max(0, xW_1 + b_1)W_2 + b_2\n",
      "\\]\n",
      "\n",
      "### 2.4. Positional Encoding\n",
      "Since attention is permutation‑invariant, the model adds **positional encodings** to the input embeddings to inject order information. The paper uses sinusoidal functions:\n",
      "\n",
      "\\[\n",
      "PE_{(pos,2i)} = \\sin\\!\\big(pos/10000^{2i/d_{\\text{model}}}\\big),\\quad\n",
      "PE_{(pos,2i+1)} = \\cos\\!\\big(pos/10000^{2i/d_{\\text{model}}}\\big)\n",
      "\\]\n",
      "\n",
      "These allow the model to extrapolate to longer sequences than seen during training.\n",
      "\n",
      "### 2.5. Encoder–Decoder Structure\n",
      "* **Encoder:** Stacks *N* identical layers, each consisting of (1) multi‑head self‑attention, (2) residual connection + layer norm, (3) FFN + residual + layer norm.  \n",
      "* **Decoder:** Similar stack, but each layer has (1) masked multi‑head self‑attention (preventing positions from attending to future tokens), (2) multi‑head encoder‑decoder attention, (3) FFN, each with residual + layer‑norm.\n",
      "\n",
      "### 2.6. Training Details\n",
      "* **Optimization:** Adam optimizer with \\(\\beta_1=0.9,\\ \\beta_2=0.98\\), learning‑rate schedule with warm‑up steps.  \n",
      "* **Regularization:** Dropout (0.1) on attention weights and FFN outputs, label smoothing (0.1).  \n",
      "* **Batching:** Byte‑pair encoding (BPE) tokenization, batches of ~25k tokens.\n",
      "\n",
      "---\n",
      "\n",
      "## 3. Key results\n",
      "\n",
      "| Task | Dataset | Metric | Transformer (base) | Prior best (RNN/CNN) |\n",
      "|------|---------|--------|--------------------|----------------------|\n",
      "| English→German translation | WMT 2014 | BLEU | **28.4** | 24.6 (RNNSearch) |\n",
      "| English→French translation | WMT 2014 | BLEU | **38.4** | 34.5 (ConvS2S) |\n",
      "| Training speed (tokens/sec) | – | – | **≈ 3× faster** than comparable LSTM models (due to parallelism) |\n",
      "\n",
      "The base model (6 encoder + 6 decoder layers, \\(d_{\\text{model}}=512\\), 8 heads) already outperformed the strongest published systems, while a larger “big” model (12 layers, \\(d_{\\text{model}}=1024\\)) achieved even higher scores.\n",
      "\n",
      "---\n",
      "\n",
      "## 4. Why it matters – impact on the field\n",
      "\n",
      "1. **Parallelism:** Self‑attention allows all positions to be processed simultaneously, dramatically reducing training time on GPUs/TPUs.  \n",
      "2. **Long‑range dependencies:** Each token can directly attend to any other token, making it easier to capture global context.  \n",
      "3. **Modularity:** The same attention block is reused in encoder, decoder, and even in purely encoder‑only or decoder‑only models.  \n",
      "4. **Catalyst for a wave of models:** BERT, GPT, RoBERTa, T5, Vision Transformers (ViT), and countless domain‑specific variants all inherit the core attention mechanism introduced here.  \n",
      "5. **Simplified architecture:** By removing recurrence and convolution, the model is conceptually cleaner and easier to extend (e.g., adding relative positional encodings, sparse attention, etc.).\n",
      "\n",
      "---\n",
      "\n",
      "## 5. Notable follow‑up work & extensions\n",
      "\n",
      "| Paper | Year | Main contribution |\n",
      "|-------|------|-------------------|\n",
      "| **BERT: Pre‑training of Deep Bidirectional Transformers for Language Understanding** | 2018 | Introduced masked language modeling and next‑sentence prediction pre‑training on the Transformer encoder. |\n",
      "| **GPT‑2 / GPT‑3** | 2019‑2020 | Demonstrated large‑scale, decoder‑only Transformers for generative language modeling. |\n",
      "| **Transformer‑XL** | 2019 | Added recurrence across segments and relative positional encodings to handle longer contexts. |\n",
      "| **Sparse Transformers / Longformer / Reformer** | 2019‑2020 | Developed efficient attention patterns (e.g., locality‑sensitive hashing, sliding windows) to scale to very long sequences. |\n",
      "| **Vision Transformer (ViT)** | 2020 | Applied pure Transformer to image patches, achieving competitive performance with CNNs. |\n",
      "| **T5 (Text‑to‑Text Transfer Transformer)** | 2020 | Unified many NLP tasks under a single encoder‑decoder framework, showing the flexibility of the original architecture. |\n",
      "\n",
      "---\n",
      "\n",
      "## 6. Quick cheat‑sheet for implementing a vanilla Transformer\n",
      "\n",
      "```python\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "import math\n",
      "\n",
      "class MultiHeadAttention(nn.Module):\n",
      "    def __init__(self, d_model, n_heads):\n",
      "        super().__init__()\n",
      "        assert d_model % n_heads == 0\n",
      "        self.d_k = d_model // n_heads\n",
      "        self.n_heads = n_heads\n",
      "\n",
      "        self.W_q = nn.Linear(d_model, d_model)\n",
      "        self.W_k = nn.Linear(d_model, d_model)\n",
      "        self.W_v = nn.Linear(d_model, d_model)\n",
      "        self.W_o = nn.Linear(d_model, d_model)\n",
      "\n",
      "    def forward(self, q, k, v, mask=None):\n",
      "        batch = q.size(0)\n",
      "\n",
      "        # Linear projections\n",
      "        Q = self.W_q(q).view(batch, -1, self.n_heads, self.d_k).transpose(1,2)\n",
      "        K = self.W_k(k).view(batch, -1, self.n_heads, self.d_k).transpose(1,2)\n",
      "        V = self.W_v(v).view(batch, -1, self.n_heads, self.d_k).transpose(1,2)\n",
      "\n",
      "        # Scaled dot‑product\n",
      "        scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.d_k)\n",
      "        if mask is not None:\n",
      "            scores = scores.masked_fill(mask == 0, -1e9)\n",
      "        attn = torch.softmax(scores, dim=-1)\n",
      "\n",
      "        # Weighted sum\n",
      "        context = torch.matmul(attn, V)          # (batch, heads, seq_len, d_k)\n",
      "        context = context.transpose(1,2).contiguous().view(batch, -1, self.n_heads*self.d_k)\n",
      "        return self.W_o(context)\n",
      "\n",
      "class PositionwiseFFN(nn.Module):\n",
      "    def __init__(self, d_model, d_ff, dropout=0.1):\n",
      "        super().__init__()\n",
      "        self.net = nn.Sequential(\n",
      "            nn.Linear(d_model, d_ff),\n",
      "            nn.ReLU(),\n",
      "            nn.Dropout(dropout),\n",
      "            nn.Linear(d_ff, d_model),\n",
      "            nn.Dropout(dropout)\n",
      "        )\n",
      "    def forward(self, x):\n",
      "        return self.net(x)\n",
      "\n",
      "class EncoderLayer(nn.Module):\n",
      "    def __init__(self, d_model, n_heads, d_ff, dropout=0.1):\n",
      "        super().__init__()\n",
      "        self.self_attn = MultiHeadAttention(d_model, n_heads)\n",
      "        self.ffn = PositionwiseFFN(d_model, d_ff, dropout)\n",
      "        self.norm1 = nn.LayerNorm(d_model)\n",
      "        self.norm2 = nn.LayerNorm(d_model)\n",
      "        self.dropout = nn.Dropout(dropout)\n",
      "\n",
      "    def forward(self, src, src_mask=None):\n",
      "        # Self‑attention\n",
      "        attn_out = self.self_attn(src, src, src, src_mask)\n",
      "        src = self.norm1(src + self.dropout(attn_out))\n",
      "        # Feed‑forward\n",
      "        ffn_out = self.ffn(src)\n",
      "        src = self.norm2(src + self.dropout(ffn_out))\n",
      "        return src\n",
      "```\n",
      "\n",
      "*(The decoder layer adds a second masked self‑attention and an encoder‑decoder attention block; the full model stacks 6–12 such layers.)*\n",
      "\n",
      "---\n",
      "\n",
      "## 7. TL;DR\n",
      "\n",
      "*The Transformer introduced a **pure attention‑based** architecture that eliminates recurrence and convolution, enabling massive parallelism and superior handling of long‑range dependencies. Its simple, modular design achieved state‑of‑the‑art translation results in 2017 and has since become the foundational building block for virtually all modern large‑scale language, vision, and multimodal models.*\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "messages = graph.invoke({\"messages\" : HumanMessage(content=\"1706.03762\")},config=config)\n",
    "for m in messages['messages'] : \n",
    "    pprint(m.pretty_print())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "fb2c7903",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "1706.03762\n",
      "None\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Tool Calls:\n",
      "  arxiv (fc_f1828abb-3273-4a0e-b207-c5ff85cebf97)\n",
      " Call ID: fc_f1828abb-3273-4a0e-b207-c5ff85cebf97\n",
      "  Args:\n",
      "    query: 1706.03762\n",
      "None\n",
      "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
      "Name: arxiv\n",
      "\n",
      "Published: 2023-08-02\n",
      "Title: Attention Is All You Need\n",
      "Authors: Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin\n",
      "Summary: The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, base\n",
      "None\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "**Paper:** *Attention Is All You Need*  \n",
      "**arXiv ID:** [1706.03762](https://arxiv.org/abs/1706.03762)  \n",
      "**Authors:** Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin  \n",
      "**Submitted:** June 2017 (published in *NeurIPS 2017*)  \n",
      "\n",
      "---\n",
      "\n",
      "## 1. What problem does the paper address?\n",
      "\n",
      "Sequence‑to‑sequence (seq2seq) models for tasks such as machine translation, summarization, and speech recognition have traditionally relied on **recurrent neural networks (RNNs)** or **convolutional neural networks (CNNs)**.  \n",
      "These architectures suffer from:\n",
      "\n",
      "* **Limited parallelism** – RNNs process tokens sequentially, slowing training.  \n",
      "* **Long‑range dependency bottlenecks** – Capturing relationships between distant tokens requires many recurrent steps.  \n",
      "* **Complexity of stacking many layers** – Deep RNNs become hard to train and memory‑intensive.\n",
      "\n",
      "The authors ask: *Can we dispense with recurrence and convolution altogether and still achieve state‑of‑the‑art performance?*  \n",
      "\n",
      "---\n",
      "\n",
      "## 2. Core idea – the **Transformer** architecture\n",
      "\n",
      "The Transformer replaces recurrence and convolution with **self‑attention** (also called *scaled dot‑product attention*) as the sole means of mixing information across positions.\n",
      "\n",
      "### 2.1. Scaled Dot‑Product Attention\n",
      "Given queries **Q**, keys **K**, and values **V**:\n",
      "\n",
      "\\[\n",
      "\\text{Attention}(Q,K,V)=\\text{softmax}\\!\\Big(\\frac{QK^{\\top}}{\\sqrt{d_k}}\\Big)V\n",
      "\\]\n",
      "\n",
      "* The dot‑product measures similarity between a query and each key.  \n",
      "* Division by \\(\\sqrt{d_k}\\) stabilizes gradients.  \n",
      "* Softmax yields a distribution over all positions, allowing each token to attend to every other token.\n",
      "\n",
      "### 2.2. Multi‑Head Attention\n",
      "Instead of a single attention function, the model uses **h** parallel heads:\n",
      "\n",
      "\\[\n",
      "\\text{MultiHead}(Q,K,V)=\\text{Concat}(\\text{head}_1,\\dots,\\text{head}_h)W^O\n",
      "\\]\n",
      "\n",
      "Each head linearly projects Q, K, V into different subspaces, learns distinct relational patterns, and the concatenated result is projected back to the model dimension.\n",
      "\n",
      "### 2.3. Position‑wise Feed‑Forward Networks (FFN)\n",
      "After attention, each position passes through the same two‑layer fully‑connected network:\n",
      "\n",
      "\\[\n",
      "\\text{FFN}(x)=\\max(0, xW_1 + b_1)W_2 + b_2\n",
      "\\]\n",
      "\n",
      "### 2.4. Positional Encoding\n",
      "Since attention is permutation‑invariant, the model adds **positional encodings** to the input embeddings to inject order information. The paper uses sinusoidal functions:\n",
      "\n",
      "\\[\n",
      "PE_{(pos,2i)} = \\sin\\!\\big(pos/10000^{2i/d_{\\text{model}}}\\big),\\quad\n",
      "PE_{(pos,2i+1)} = \\cos\\!\\big(pos/10000^{2i/d_{\\text{model}}}\\big)\n",
      "\\]\n",
      "\n",
      "These allow the model to extrapolate to longer sequences than seen during training.\n",
      "\n",
      "### 2.5. Encoder–Decoder Structure\n",
      "* **Encoder:** Stacks *N* identical layers, each consisting of (1) multi‑head self‑attention, (2) residual connection + layer norm, (3) FFN + residual + layer norm.  \n",
      "* **Decoder:** Similar stack, but each layer has (1) masked multi‑head self‑attention (preventing positions from attending to future tokens), (2) multi‑head encoder‑decoder attention, (3) FFN, each with residual + layer‑norm.\n",
      "\n",
      "### 2.6. Training Details\n",
      "* **Optimization:** Adam optimizer with \\(\\beta_1=0.9,\\ \\beta_2=0.98\\), learning‑rate schedule with warm‑up steps.  \n",
      "* **Regularization:** Dropout (0.1) on attention weights and FFN outputs, label smoothing (0.1).  \n",
      "* **Batching:** Byte‑pair encoding (BPE) tokenization, batches of ~25k tokens.\n",
      "\n",
      "---\n",
      "\n",
      "## 3. Key results\n",
      "\n",
      "| Task | Dataset | Metric | Transformer (base) | Prior best (RNN/CNN) |\n",
      "|------|---------|--------|--------------------|----------------------|\n",
      "| English→German translation | WMT 2014 | BLEU | **28.4** | 24.6 (RNNSearch) |\n",
      "| English→French translation | WMT 2014 | BLEU | **38.4** | 34.5 (ConvS2S) |\n",
      "| Training speed (tokens/sec) | – | – | **≈ 3× faster** than comparable LSTM models (due to parallelism) |\n",
      "\n",
      "The base model (6 encoder + 6 decoder layers, \\(d_{\\text{model}}=512\\), 8 heads) already outperformed the strongest published systems, while a larger “big” model (12 layers, \\(d_{\\text{model}}=1024\\)) achieved even higher scores.\n",
      "\n",
      "---\n",
      "\n",
      "## 4. Why it matters – impact on the field\n",
      "\n",
      "1. **Parallelism:** Self‑attention allows all positions to be processed simultaneously, dramatically reducing training time on GPUs/TPUs.  \n",
      "2. **Long‑range dependencies:** Each token can directly attend to any other token, making it easier to capture global context.  \n",
      "3. **Modularity:** The same attention block is reused in encoder, decoder, and even in purely encoder‑only or decoder‑only models.  \n",
      "4. **Catalyst for a wave of models:** BERT, GPT, RoBERTa, T5, Vision Transformers (ViT), and countless domain‑specific variants all inherit the core attention mechanism introduced here.  \n",
      "5. **Simplified architecture:** By removing recurrence and convolution, the model is conceptually cleaner and easier to extend (e.g., adding relative positional encodings, sparse attention, etc.).\n",
      "\n",
      "---\n",
      "\n",
      "## 5. Notable follow‑up work & extensions\n",
      "\n",
      "| Paper | Year | Main contribution |\n",
      "|-------|------|-------------------|\n",
      "| **BERT: Pre‑training of Deep Bidirectional Transformers for Language Understanding** | 2018 | Introduced masked language modeling and next‑sentence prediction pre‑training on the Transformer encoder. |\n",
      "| **GPT‑2 / GPT‑3** | 2019‑2020 | Demonstrated large‑scale, decoder‑only Transformers for generative language modeling. |\n",
      "| **Transformer‑XL** | 2019 | Added recurrence across segments and relative positional encodings to handle longer contexts. |\n",
      "| **Sparse Transformers / Longformer / Reformer** | 2019‑2020 | Developed efficient attention patterns (e.g., locality‑sensitive hashing, sliding windows) to scale to very long sequences. |\n",
      "| **Vision Transformer (ViT)** | 2020 | Applied pure Transformer to image patches, achieving competitive performance with CNNs. |\n",
      "| **T5 (Text‑to‑Text Transfer Transformer)** | 2020 | Unified many NLP tasks under a single encoder‑decoder framework, showing the flexibility of the original architecture. |\n",
      "\n",
      "---\n",
      "\n",
      "## 6. Quick cheat‑sheet for implementing a vanilla Transformer\n",
      "\n",
      "```python\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "import math\n",
      "\n",
      "class MultiHeadAttention(nn.Module):\n",
      "    def __init__(self, d_model, n_heads):\n",
      "        super().__init__()\n",
      "        assert d_model % n_heads == 0\n",
      "        self.d_k = d_model // n_heads\n",
      "        self.n_heads = n_heads\n",
      "\n",
      "        self.W_q = nn.Linear(d_model, d_model)\n",
      "        self.W_k = nn.Linear(d_model, d_model)\n",
      "        self.W_v = nn.Linear(d_model, d_model)\n",
      "        self.W_o = nn.Linear(d_model, d_model)\n",
      "\n",
      "    def forward(self, q, k, v, mask=None):\n",
      "        batch = q.size(0)\n",
      "\n",
      "        # Linear projections\n",
      "        Q = self.W_q(q).view(batch, -1, self.n_heads, self.d_k).transpose(1,2)\n",
      "        K = self.W_k(k).view(batch, -1, self.n_heads, self.d_k).transpose(1,2)\n",
      "        V = self.W_v(v).view(batch, -1, self.n_heads, self.d_k).transpose(1,2)\n",
      "\n",
      "        # Scaled dot‑product\n",
      "        scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.d_k)\n",
      "        if mask is not None:\n",
      "            scores = scores.masked_fill(mask == 0, -1e9)\n",
      "        attn = torch.softmax(scores, dim=-1)\n",
      "\n",
      "        # Weighted sum\n",
      "        context = torch.matmul(attn, V)          # (batch, heads, seq_len, d_k)\n",
      "        context = context.transpose(1,2).contiguous().view(batch, -1, self.n_heads*self.d_k)\n",
      "        return self.W_o(context)\n",
      "\n",
      "class PositionwiseFFN(nn.Module):\n",
      "    def __init__(self, d_model, d_ff, dropout=0.1):\n",
      "        super().__init__()\n",
      "        self.net = nn.Sequential(\n",
      "            nn.Linear(d_model, d_ff),\n",
      "            nn.ReLU(),\n",
      "            nn.Dropout(dropout),\n",
      "            nn.Linear(d_ff, d_model),\n",
      "            nn.Dropout(dropout)\n",
      "        )\n",
      "    def forward(self, x):\n",
      "        return self.net(x)\n",
      "\n",
      "class EncoderLayer(nn.Module):\n",
      "    def __init__(self, d_model, n_heads, d_ff, dropout=0.1):\n",
      "        super().__init__()\n",
      "        self.self_attn = MultiHeadAttention(d_model, n_heads)\n",
      "        self.ffn = PositionwiseFFN(d_model, d_ff, dropout)\n",
      "        self.norm1 = nn.LayerNorm(d_model)\n",
      "        self.norm2 = nn.LayerNorm(d_model)\n",
      "        self.dropout = nn.Dropout(dropout)\n",
      "\n",
      "    def forward(self, src, src_mask=None):\n",
      "        # Self‑attention\n",
      "        attn_out = self.self_attn(src, src, src, src_mask)\n",
      "        src = self.norm1(src + self.dropout(attn_out))\n",
      "        # Feed‑forward\n",
      "        ffn_out = self.ffn(src)\n",
      "        src = self.norm2(src + self.dropout(ffn_out))\n",
      "        return src\n",
      "```\n",
      "\n",
      "*(The decoder layer adds a second masked self‑attention and an encoder‑decoder attention block; the full model stacks 6–12 such layers.)*\n",
      "\n",
      "---\n",
      "\n",
      "## 7. TL;DR\n",
      "\n",
      "*The Transformer introduced a **pure attention‑based** architecture that eliminates recurrence and convolution, enabling massive parallelism and superior handling of long‑range dependencies. Its simple, modular design achieved state‑of‑the‑art translation results in 2017 and has since become the foundational building block for virtually all modern large‑scale language, vision, and multimodal models.*\n",
      "None\n",
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "what is the company behind it ?\n",
      "None\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "**The Transformer was invented at Google.**  \n",
      "\n",
      "- **Affiliation of the authors:** All eight authors (Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin) list **Google Brain** (and more generally “Google Research”) as their institutional affiliation on the arXiv pre‑print and in the conference proceedings.  \n",
      "\n",
      "- **Research group:** The work was carried out within **Google Brain**, the deep‑learning research division of Google that collaborates closely with the broader Google Research organization.  \n",
      "\n",
      "- **Why Google?** Google had already been pioneering large‑scale sequence‑to‑sequence models (e.g., the “Neural Machine Translation” system) and was looking for a more efficient architecture. The Transformer was the result of that internal effort, and the paper was submitted to *NeurIPS 2017* while the authors were employees of Google.\n",
      "\n",
      "So, the company behind the original “Attention Is All You Need” paper is **Google (Google Brain / Google Research)**.\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "messages = graph.invoke({\"messages\" : HumanMessage(content=\"what is the company behind it ?\")},config=config)\n",
    "for m in messages['messages'] : \n",
    "    pprint(m.pretty_print())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lc-academy-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
